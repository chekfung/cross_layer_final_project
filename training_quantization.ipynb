{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chekfung/cross_layer_final_project/blob/main/training_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWCSJNoXXjMU"
      },
      "source": [
        "# Source Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJuqknGwXjMW"
      },
      "source": [
        "## Imports Needed Throughout the Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "B-AzI0DdXjMY"
      },
      "outputs": [],
      "source": [
        "# All Imports \n",
        "import sys\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from google.colab import files\n",
        "\n",
        "# argument parser\n",
        "import easydict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItgEiUWPXjMa"
      },
      "source": [
        "## Get and Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wWPbK4uKXjMb"
      },
      "outputs": [],
      "source": [
        "# MNIST Dataset (Images and Labels)\n",
        "train_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()                                 \n",
        "    ])\n",
        ")\n",
        "\n",
        "test_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()                                 \n",
        "    ])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOYytU77XjMc"
      },
      "source": [
        "## More Helper Code for Training and Testing Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "QcNG3HYC7dJk"
      },
      "outputs": [],
      "source": [
        "def fuse_conv_bn(model,out_model):\n",
        "\n",
        "  conv_layer = None\n",
        "  count = 0\n",
        "\n",
        "  # 1. for loop to collect all Conv layers\n",
        "  # 2. for loop to collect all BatchNorm layers\n",
        "  for layer in model.modules():\n",
        "    \n",
        "    if isinstance(layer, nn.BatchNorm2d):\n",
        "\n",
        "      conv_size = conv_layer.weight.size()\n",
        "\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for i in range(conv_size[0]):\n",
        "\n",
        "          \n",
        "          # get the conv2d weights\n",
        "          weights = conv_layer.weight[i]\n",
        "\n",
        "\n",
        "          denominator = torch.sqrt(layer.eps+layer.running_var)\n",
        "          gamma = layer.weight[i]\n",
        "          beta = layer.bias[i]\n",
        "          \n",
        "\n",
        "          for j in range(conv_size[1]):\n",
        "            for k in range(conv_size[2]):\n",
        "              for l in range(conv_size[3]):\n",
        "                # update out_model layer[count]\n",
        "\n",
        "                if count == 0:\n",
        "                  out_model.conv1.weight[i][j][k][l] = gamma * conv_layer.weight[i][j][k][l]  / denominator[i]  \n",
        "                else:\n",
        "                  out_model.conv2.weight[i][j][k][l] = gamma * conv_layer.weight[i][j][k][l]  / denominator[i]  \n",
        "\n",
        "          # In i loop for bias since only 1D\n",
        "          if count == 0:\n",
        "            out_model.conv1.bias[i] = (gamma * (conv_layer.bias[i] - layer.running_mean[i])  / denominator[i]) + beta\n",
        "          else:\n",
        "            out_model.conv2.bias[i] = (gamma * (conv_layer.bias[i] - layer.running_mean[i])  / denominator[i]) + beta\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    conv_layer = layer # conv2d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzSo2kcrXjMe"
      },
      "source": [
        "## FP32 Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "vlRcw7IMXjMg"
      },
      "outputs": [],
      "source": [
        "class MyConvNet_FP32(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MyConvNet_FP32, self).__init__()\n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(16)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(32)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.lin2  = nn.Linear(7*7*32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        c1 = self.conv1(x)\n",
        "        b1  = self.bn1(c1)\n",
        "        a1  = self.act1(b1)\n",
        "        p1  = self.pool1(a1)\n",
        "\n",
        "        # Layer 2\n",
        "        c2  = self.conv2(p1)\n",
        "        b2  = self.bn2(c2)\n",
        "        a2  = self.act2(b2)\n",
        "        p2  = self.pool2(a2)\n",
        "\n",
        "        # Flatten and Layer 3\n",
        "        flt = p2.view(p2.size(0), -1)\n",
        "        out = self.lin2(flt)\n",
        "        return out\n",
        "  \n",
        "# model = MyConvNet(args)\n",
        "# model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlSJ4lJwXjMh"
      },
      "source": [
        "## Quantization Helper Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "I57BN8KQXjMi"
      },
      "outputs": [],
      "source": [
        "def simple_quantize_val(val, scale_factor, min_val, max_val):\n",
        "  value = torch.round(val / scale_factor)\n",
        "\n",
        "  if (value < min_val):\n",
        "    value = min_val\n",
        "\n",
        "  if (value > max_val):\n",
        "    value = max_val\n",
        "\n",
        "  return (value * scale_factor)\n",
        "\n",
        "def fixed_point_quantize_val(val, num_bits, fractional_bits):\n",
        "  integer_bits = num_bits - fractional_bits - 1 # Subtract one for sign bit\n",
        "  smallest_step_size = 1 / np.power(2, fractional_bits)\n",
        "  largest_number = (np.power(2, integer_bits) - 1) + ((np.power(2, fractional_bits)-1) * smallest_step_size)\n",
        "  smallest_number = -1 * np.power(2, integer_bits)\n",
        "\n",
        "  value = torch.round(val / smallest_step_size) * smallest_step_size\n",
        "\n",
        "  if (value < smallest_number):\n",
        "    value = smallest_number\n",
        "\n",
        "  if (value > largest_number):\n",
        "    value = largest_number\n",
        "\n",
        "  return value\n",
        "\n",
        "# Perhaps slightly optimized version?\n",
        "def fixed_point_quantize_faster(val, smallest_step_size, largest_number, smallest_number):\n",
        "\n",
        "  # Perform Pseudo Quantization\n",
        "  value = torch.round(val / smallest_step_size) * smallest_step_size\n",
        "\n",
        "  # Clamp Values\n",
        "  if (value < smallest_number):\n",
        "    value = smallest_number\n",
        "\n",
        "  if (value > largest_number):\n",
        "    value = largest_number\n",
        "\n",
        "  return value\n",
        "\n",
        "### NOTE: ONLY USE THIS SHIT ###\n",
        "def optimized_tensor_fp_quantize(tens, step_size, largest_num, smallest_num):\n",
        "  # Do everything in tensor operations\n",
        "  new_tensor = torch.round(tens / step_size) * step_size\n",
        "  torch.clamp(new_tensor, min=smallest_num, max=largest_num) \n",
        "  return new_tensor\n",
        "\n",
        "# Right now, only for integer quantization.\n",
        "def quantize_fp_model_weights(model):\n",
        "  count = 0\n",
        "  for layer in model.modules():\n",
        "    if not isinstance(layer, (nn.ReLU, nn.MaxPool2d)) and count != 0:\n",
        "        \n",
        "      with torch.no_grad():\n",
        "        layer.weight.data = optimized_tensor_fp_quantize(layer.weight.data, model.weights_step_size, model.weights_largest_num_representable, model.weights_smallest_num_representable)\n",
        "\n",
        "    count += 1\n",
        "  \n",
        "  return 0\n",
        "\n",
        "def quantize_fp_model_biases(model):\n",
        "  count = 0\n",
        "  for layer in model.modules():\n",
        "    if not isinstance(layer, (nn.ReLU, nn.MaxPool2d)) and count != 0:\n",
        "        \n",
        "      with torch.no_grad():\n",
        "        layer.bias.data = optimized_tensor_fp_quantize(layer.bias.data, model.bias_step_size, model.bias_largest_num_representable, model.bias_smallest_num_representable)\n",
        "\n",
        "    count += 1\n",
        "  \n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ln6bdN1fXjMd"
      },
      "outputs": [],
      "source": [
        "def train_model(arg, model, criterion, optimizer, train_loader, quantize=False):\n",
        "    print(\"---Training started\")\n",
        "    # Training the Model\n",
        "    for epoch in range(arg.epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Load Images into GPU\n",
        "            images = images.cuda()\n",
        "            labels = Variable(labels).cuda()\n",
        "\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            L1norm = model.parameters()\n",
        "            arr = []\n",
        "\n",
        "            # Calculate L1 Norm (if included in hyperparameters)\n",
        "            if arg.L1norm == True:\n",
        "                for name,param in model.named_parameters():\n",
        "                    if 'weight' in name.split('.'):\n",
        "                        arr.append(param)\n",
        "\n",
        "                L1loss = 0\n",
        "                for Losstmp in arr:\n",
        "                    L1loss = L1loss+Losstmp.abs().mean()\n",
        "\n",
        "                if len(arr) > 0:\n",
        "                    loss = loss+L1loss/len(arr)\n",
        "\n",
        "            if quantize:\n",
        "                # quantize loss\n",
        "                loss.data = optimized_tensor_fp_quantize(loss.data, model.loss_step_size, model.loss_largest_num_representable, model.loss_smallest_num_representable)\n",
        "\n",
        "            # Optimizer Step, Propagate Loss backwards\n",
        "            loss.backward()\n",
        "\n",
        "            if quantize:\n",
        "              \n",
        "                # quantize gradients\n",
        "                for name,param in model.named_parameters():\n",
        "                    # Print BEFORE Gradients\n",
        "                    # print(gradient)\n",
        "\n",
        "                    param.grad.data = optimized_tensor_fp_quantize(param.grad.data, model.gradient_step_size, model.gradient_largest_num_representable, model.gradient_smallest_num_representable)\n",
        "                    \n",
        "                    # Print AFTER Gradients\n",
        "                    # print(gradient)\n",
        "                \n",
        "            optimizer.step()\n",
        "\n",
        "            # # TODO: Need to quantize the biases as well.\n",
        "            if quantize:\n",
        "              quantize_fp_model_weights(model)\n",
        "              quantize_fp_model_biases(model)\n",
        "\n",
        "            if (i + 1) % 600 == 0:\n",
        "                print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                        % (epoch + 1, arg.epochs, i + 1,\n",
        "                        len(train_set) // arg.batch_size, loss.data.item()))\n",
        "\n",
        "\n",
        "# Gets accuracy given dataset as well as total test loss\n",
        "def get_acc(model, criterion, test_loader, quantized=False):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = model(images)\n",
        "        testloss = criterion(outputs, labels)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "        break\n",
        "\n",
        "    if quantized:\n",
        "      testloss = optimized_tensor_fp_quantize(testloss, model.loss_step_size, model.loss_largest_num_representable, model.loss_smallest_num_representable)\n",
        "\n",
        "    return ((100 * correct / total), testloss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLM8kj7KXjMi"
      },
      "source": [
        "## Quantization Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "YQn2A92SXjMj"
      },
      "outputs": [],
      "source": [
        "# Assume that we always \n",
        "class MyConvNet_FIXED_POINT(nn.Module):\n",
        "    def __init__(self, args, forwardfp, gradientfp, lossfp, weightsfp, biasfp): #xfp's are tuples of the # of bits in the fp rep & # of fractional bits\n",
        "        super(MyConvNet_FIXED_POINT, self).__init__()\n",
        "\n",
        "\n",
        "        ### gradient FP Parameters ###\n",
        "        self.gradient_fp_bits = gradientfp[0]\n",
        "        self.gradient_int_bits = (gradientfp[0] - 1 - gradientfp[1])\n",
        "        self.gradient_fract_bits = gradientfp[1]\n",
        " \n",
        "        self.gradient_step_size = 1 / np.power(2, gradientfp[1])\n",
        "        self.gradient_largest_num_representable = (np.power(2, self.gradient_int_bits) - 1) + ((np.power(2, self.gradient_fract_bits)-1) * self.gradient_step_size)\n",
        "        self.gradient_smallest_num_representable = -1 * np.power(2, self.gradient_int_bits)\n",
        "        #########################\n",
        "\n",
        "        ### forward FP Parameters ###\n",
        "        self.forward_fp_bits = forwardfp[0]\n",
        "        self.forward_int_bits = (forwardfp[0] - 1 - forwardfp[1])\n",
        "        self.forward_fract_bits = forwardfp[1]\n",
        " \n",
        "        self.forward_step_size = 1 / np.power(2, forwardfp[1])\n",
        "        self.forward_largest_num_representable = (np.power(2, self.forward_int_bits) - 1) + ((np.power(2, self.forward_fract_bits)-1) * self.forward_step_size)\n",
        "        self.forward_smallest_num_representable = -1 * np.power(2, self.forward_int_bits)\n",
        "        #########################\n",
        "\n",
        "        ### Loss FP Parameters ###\n",
        "        self.loss_fp_bits = lossfp[0]\n",
        "        self.loss_int_bits = (lossfp[0] - 1 - lossfp[1])\n",
        "        self.loss_fract_bits = lossfp[1]\n",
        "\n",
        "        self.loss_step_size = 1 / np.power(2, lossfp[1])\n",
        "        self.loss_largest_num_representable = (np.power(2, self.loss_int_bits) - 1) + ((np.power(2, self.loss_fract_bits)-1) * self.loss_step_size)\n",
        "        self.loss_smallest_num_representable = -1 * np.power(2, self.loss_int_bits)\n",
        "        #########################\n",
        "\n",
        "        ### weights FP Parameters ###\n",
        "        self.weights_fp_bits = weightsfp[0]\n",
        "        self.weights_int_bits = (weightsfp[0] - 1 - weightsfp[1])\n",
        "        self.weights_fract_bits = weightsfp[1]\n",
        " \n",
        "        self.weights_step_size = 1 / np.power(2, weightsfp[1])\n",
        "        self.weights_largest_num_representable = (np.power(2, self.weights_int_bits) - 1) + ((np.power(2, self.weights_fract_bits)-1) * self.weights_step_size)\n",
        "        self.weights_smallest_num_representable = -1 * np.power(2, self.weights_int_bits)\n",
        "        #########################\n",
        "\n",
        "        ### bias FP Parameters ###\n",
        "        self.bias_fp_bits = biasfp[0]\n",
        "        self.bias_int_bits = (biasfp[0] - 1 - biasfp[1])\n",
        "        self.bias_fract_bits = biasfp[1]\n",
        " \n",
        "        self.bias_step_size = 1 / np.power(2, biasfp[1])\n",
        "        self.bias_largest_num_representable = (np.power(2, self.bias_int_bits) - 1) + ((np.power(2, self.bias_fract_bits)-1) * self.bias_step_size)\n",
        "        self.bias_smallest_num_representable = -1 * np.power(2, self.bias_int_bits)\n",
        "        #########################\n",
        "\n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(16)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(32)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.lin2  = nn.Linear(7*7*32, 10)\n",
        "\n",
        "    # AUGMENT FORWARD PASS. forward pass not quantizes every single time we go through\n",
        "    def forward(self, x):\n",
        "        xq = optimized_tensor_fp_quantize(x, self.forward_step_size, self.forward_largest_num_representable, self.forward_smallest_num_representable)\n",
        "        # Layer 1\n",
        "        c1 = self.conv1(xq)\n",
        "        c1q = optimized_tensor_fp_quantize(c1, self.forward_step_size, self.forward_largest_num_representable, self.forward_smallest_num_representable)\n",
        "        b1  = self.bn1(c1q)\n",
        "        b1q = optimized_tensor_fp_quantize(b1, self.forward_step_size, self.forward_largest_num_representable, self.forward_smallest_num_representable)\n",
        "        a1  = self.act1(b1q)\n",
        "        p1  = self.pool1(a1)\n",
        " \n",
        "        # Layer 2\n",
        "        c2  = self.conv2(p1)\n",
        "        c2q = optimized_tensor_fp_quantize(c2, self.forward_step_size, self.forward_largest_num_representable, self.forward_smallest_num_representable)\n",
        "        b2  = self.bn2(c2q)\n",
        "        b2q = optimized_tensor_fp_quantize(b2, self.forward_step_size, self.forward_largest_num_representable, self.forward_smallest_num_representable)\n",
        "        a2  = self.act2(b2q)\n",
        "        p2  = self.pool2(a2)\n",
        " \n",
        "        # Flatten and Layer 3\n",
        "        flt = p2.view(p2.size(0), -1)\n",
        "        out = self.lin2(flt)\n",
        "        out_new = optimized_tensor_fp_quantize(out, self.forward_step_size, self.forward_largest_num_representable, self.forward_smallest_num_representable)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "8t8Zej6m3ufg"
      },
      "outputs": [],
      "source": [
        "class QuantizedConvNet(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedConvNet, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        # FP32 model\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGjcFMQ5XjMj"
      },
      "source": [
        "# Test Code Here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "RjT2c_f9XjMk"
      },
      "outputs": [],
      "source": [
        "# Hyper Parameter for FashionMNIST\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example of instantiating a model, training it with given hyperparameters, etc"
      ],
      "metadata": {
        "id": "5kUXftre3oYu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IlnS6XwxXjMk",
        "outputId": "e9586ea2-2f58-456a-a31b-e726d95ea82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "32\n",
            "---Training started\n",
            "Epoch: [ 1/ 1], Step: [ 600/ 3750], Loss: 0.3976\n",
            "Epoch: [ 1/ 1], Step: [ 1200/ 3750], Loss: 0.3463\n",
            "Epoch: [ 1/ 1], Step: [ 1800/ 3750], Loss: 1.1999\n",
            "Epoch: [ 1/ 1], Step: [ 2400/ 3750], Loss: 0.4780\n",
            "Epoch: [ 1/ 1], Step: [ 3000/ 3750], Loss: 0.3860\n",
            "Epoch: [ 1/ 1], Step: [ 3600/ 3750], Loss: 1.0294\n",
            "Test Accuracy: 87.5\n",
            "Test Loss: 0.2588944137096405\n",
            "32\n",
            "16\n",
            "---Training started\n",
            "Epoch: [ 1/ 1], Step: [ 600/ 3750], Loss: 0.2637\n",
            "Epoch: [ 1/ 1], Step: [ 1200/ 3750], Loss: 0.3453\n",
            "Epoch: [ 1/ 1], Step: [ 1800/ 3750], Loss: 0.4897\n",
            "Epoch: [ 1/ 1], Step: [ 2400/ 3750], Loss: 0.0842\n",
            "Epoch: [ 1/ 1], Step: [ 3000/ 3750], Loss: 0.5559\n",
            "Epoch: [ 1/ 1], Step: [ 3600/ 3750], Loss: 0.5215\n",
            "Test Accuracy: 87.5\n",
            "Test Loss: 0.36041259765625\n",
            "16\n",
            "8\n",
            "---Training started\n",
            "Epoch: [ 1/ 1], Step: [ 600/ 3750], Loss: 2.6016\n",
            "Epoch: [ 1/ 1], Step: [ 1200/ 3750], Loss: 1.9648\n",
            "Epoch: [ 1/ 1], Step: [ 1800/ 3750], Loss: 2.0898\n",
            "Epoch: [ 1/ 1], Step: [ 2400/ 3750], Loss: 2.1602\n",
            "Epoch: [ 1/ 1], Step: [ 3000/ 3750], Loss: 2.3594\n",
            "Epoch: [ 1/ 1], Step: [ 3600/ 3750], Loss: 1.9648\n",
            "Test Accuracy: 12.5\n",
            "Test Loss: 2.203125\n",
            "8\n",
            "4\n",
            "---Training started\n",
            "Epoch: [ 1/ 1], Step: [ 600/ 3750], Loss: 2.3125\n",
            "Epoch: [ 1/ 1], Step: [ 1200/ 3750], Loss: 2.3125\n",
            "Epoch: [ 1/ 1], Step: [ 1800/ 3750], Loss: 2.3125\n",
            "Epoch: [ 1/ 1], Step: [ 2400/ 3750], Loss: 2.3125\n",
            "Epoch: [ 1/ 1], Step: [ 3000/ 3750], Loss: 2.3125\n",
            "Epoch: [ 1/ 1], Step: [ 3600/ 3750], Loss: 2.3125\n",
            "Test Accuracy: 0.0\n",
            "Test Loss: 2.3125\n",
            "4\n",
            "2\n",
            "---Training started\n",
            "Epoch: [ 1/ 1], Step: [ 600/ 3750], Loss: 2.2500\n",
            "Epoch: [ 1/ 1], Step: [ 1200/ 3750], Loss: 2.2500\n",
            "Epoch: [ 1/ 1], Step: [ 1800/ 3750], Loss: 2.2500\n",
            "Epoch: [ 1/ 1], Step: [ 2400/ 3750], Loss: 2.2500\n",
            "Epoch: [ 1/ 1], Step: [ 3000/ 3750], Loss: 2.2500\n",
            "Epoch: [ 1/ 1], Step: [ 3600/ 3750], Loss: 2.2500\n",
            "Test Accuracy: 0.0\n",
            "Test Loss: 2.25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "args = easydict.EasyDict({\n",
        "        \"batch_size\": 16,\n",
        "        \"epochs\": 1,\n",
        "        \"lr\": 0.001,\n",
        "        \"enable_cuda\" : True,\n",
        "        \"L1norm\" : False,\n",
        "        \"simpleNet\" : True,\n",
        "        \"activation\" : \"relu\", #relu, tanh, sigmoid\n",
        "        \"train_curve\" : True, \n",
        "        \"optimization\" :\"Adam\"\n",
        "})\n",
        "\n",
        "# Dataset Loader (Input Pipeline)\n",
        "quantize = True\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_set, batch_size = args.batch_size, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_set, batch_size = args.batch_size, shuffle = False)\n",
        "\n",
        "num_bits_test = [64, 32, 16, 8, 4]\n",
        "num_fractional_bits = [32, 16, 8, 4, 2]\n",
        "acc = []\n",
        "\n",
        "for (num_bits,num_fractional_bits) in zip(num_bits_test, num_fractional_bits):\n",
        "  print(num_bits)\n",
        "  print(num_fractional_bits)\n",
        "  forwardfp= (num_bits, num_fractional_bits) \n",
        "  gradientfp= (num_bits, num_fractional_bits) \n",
        "  lossfp= (num_bits, num_fractional_bits) \n",
        "  weightsfp= (num_bits, num_fractional_bits) \n",
        "  biasfp=(num_bits, num_fractional_bits) \n",
        "\n",
        "  # Declare Model\n",
        "  model = MyConvNet_FIXED_POINT(args, forwardfp, gradientfp, lossfp, weightsfp, biasfp).cuda()\n",
        "  #model = MyConvNet_FP32(args).cuda()\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss().cuda()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = args.lr) \n",
        "\n",
        "  # Training\n",
        "  train_model(args, model, criterion, optimizer, train_loader, quantize)\n",
        "\n",
        "  # Need to quantize the model afterwards\n",
        "  quantize_fp_model_weights(model)\n",
        "\n",
        "  # Testing ARC\n",
        "  test_acc, test_loss = get_acc(model, criterion, test_loader, quantize)\n",
        "  print(\"Test Accuracy: {}\".format(test_acc))\n",
        "  print(\"Test Loss: {}\".format(test_loss))\n",
        "\n",
        "  acc.append(test_acc.item())\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(num_bits_test, acc)\n",
        "plt.title(\"Varying Number of Quantized Bits\")\n",
        "plt.xlabel(\"Number of Quantized Bits\")\n",
        "plt.ylabel(\"Acc (%)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z9Q0vi95Alz"
      },
      "source": [
        "## Preston's Training of Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "fgEQBWHy5FD-"
      },
      "outputs": [],
      "source": [
        "# cuda_device = torch.device(\"cuda:0\")\n",
        "# cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "# # Initial model\n",
        "# FP_model = MyConvNet_FP32(args).cuda()\n",
        "# FP_fused_model = MyConvNet_FP32(args)\n",
        "\n",
        "# # Training stuff\n",
        "# criterion = nn.CrossEntropyLoss().cuda()\n",
        "# optimizer = torch.optim.SGD(FP_model.parameters(), lr = learning_rate) \n",
        "\n",
        "# train_model(FP_model, criterion, optimizer, train_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "P63UTvuTHjKT"
      },
      "outputs": [],
      "source": [
        "# test_acc_FP, test_loss_FP = get_acc(FP_model, criterion, test_loader)\n",
        "# print(\"Test Accuracy: {}\".format(test_acc_FP))\n",
        "# print(\"Test Loss: {}\".format(test_loss_FP))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "rJYZN0ky_9i6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# copy_models(FP_model,FP_fused_model)   \n",
        "# fuse_conv_bn(FP_model,FP_fused_model)\n",
        "\n",
        "# quantized_model = QuantizedConvNet(model_fp32=FP_fused_model).to(cpu_device)\n",
        "\n",
        "# quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "# quantized_model.qconfig = quantization_config\n",
        "# torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
        "\n",
        "# quantized_model.to(cuda_device)\n",
        "# train_model(quantized_model, criterion, optimizer, train_loader)\n",
        "# # quantized_model.to(cpu_device)\n",
        "\n",
        "# # quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "QhdEVwvJHnhl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# test_acc, test_loss = get_acc(quantized_model, criterion, test_loader)\n",
        "# print(\"Test Accuracy: {}\".format(test_acc))\n",
        "# print(\"Test Loss: {}\".format(test_loss))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}