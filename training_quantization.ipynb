{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chekfung/cross_layer_final_project/blob/main/training_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWCSJNoXXjMU"
      },
      "source": [
        "# Source Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJuqknGwXjMW"
      },
      "source": [
        "## Imports Needed Throughout the Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B-AzI0DdXjMY"
      },
      "outputs": [],
      "source": [
        "# All Imports \n",
        "import sys\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from google.colab import files\n",
        "\n",
        "# argument parser\n",
        "import easydict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItgEiUWPXjMa"
      },
      "source": [
        "## Get and Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wWPbK4uKXjMb"
      },
      "outputs": [],
      "source": [
        "# MNIST Dataset (Images and Labels)\n",
        "train_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()                                 \n",
        "    ])\n",
        ")\n",
        "\n",
        "test_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()                                 \n",
        "    ])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOYytU77XjMc"
      },
      "source": [
        "## More Helper Code for Training and Testing Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def fuse_conv_bn(model,out_model):\n",
        "\n",
        "  conv_layer = None\n",
        "  count = 0\n",
        "\n",
        "  # 1. for loop to collect all Conv layers\n",
        "  # 2. for loop to collect all BatchNorm layers\n",
        "  for layer in model.modules():\n",
        "    \n",
        "    if isinstance(layer, nn.BatchNorm2d):\n",
        "\n",
        "      conv_size = conv_layer.weight.size()\n",
        "\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for i in range(conv_size[0]):\n",
        "\n",
        "          \n",
        "          # get the conv2d weights\n",
        "          weights = conv_layer.weight[i]\n",
        "\n",
        "\n",
        "          denominator = torch.sqrt(layer.eps+layer.running_var)\n",
        "          gamma = layer.weight[i]\n",
        "          beta = layer.bias[i]\n",
        "          \n",
        "\n",
        "          for j in range(conv_size[1]):\n",
        "            for k in range(conv_size[2]):\n",
        "              for l in range(conv_size[3]):\n",
        "                # update out_model layer[count]\n",
        "\n",
        "                if count == 0:\n",
        "                  out_model.conv1.weight[i][j][k][l] = gamma * conv_layer.weight[i][j][k][l]  / denominator[i]  \n",
        "                else:\n",
        "                  out_model.conv2.weight[i][j][k][l] = gamma * conv_layer.weight[i][j][k][l]  / denominator[i]  \n",
        "\n",
        "          # In i loop for bias since only 1D\n",
        "          if count == 0:\n",
        "            out_model.conv1.bias[i] = (gamma * (conv_layer.bias[i] - layer.running_mean[i])  / denominator[i]) + beta\n",
        "          else:\n",
        "            out_model.conv2.bias[i] = (gamma * (conv_layer.bias[i] - layer.running_mean[i])  / denominator[i]) + beta\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    conv_layer = layer # conv2d\n"
      ],
      "metadata": {
        "id": "QcNG3HYC7dJk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)"
      ],
      "metadata": {
        "id": "q3PX3CO2228h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ln6bdN1fXjMd"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader):\n",
        "    print(\"---Training started\")\n",
        "    # Training the Model\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Load Images into GPU\n",
        "            images = images.cuda()\n",
        "            labels = Variable(labels).cuda()\n",
        "\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            L1norm = model.parameters()\n",
        "            arr = []\n",
        "\n",
        "            # Calculate L1 Norm (if included in hyperparameters)\n",
        "            if args.L1norm == True:\n",
        "                for name,param in model.named_parameters():\n",
        "                    if 'weight' in name.split('.'):\n",
        "                        arr.append(param)\n",
        "\n",
        "                L1loss = 0\n",
        "                for Losstmp in arr:\n",
        "                    L1loss = L1loss+Losstmp.abs().mean()\n",
        "\n",
        "                if len(arr) > 0:\n",
        "                    loss = loss+L1loss/len(arr)\n",
        "\n",
        "            # Optimizer Step, Propagate Loss backwards\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i + 1) % 600 == 0:\n",
        "                print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                        % (epoch + 1, num_epochs, i + 1,\n",
        "                        len(train_set) // batch_size, loss.data.item()))\n",
        "\n",
        "\n",
        "# Gets accuracy given dataset as well as total test loss\n",
        "def get_acc(model, criterion, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = model(images)\n",
        "        testloss = criterion(outputs, labels)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "        break\n",
        "\n",
        "    return ((100 * correct / total), testloss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzSo2kcrXjMe"
      },
      "source": [
        "## FP32 Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vlRcw7IMXjMg"
      },
      "outputs": [],
      "source": [
        "class MyConvNet_FP32(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MyConvNet_FP32, self).__init__()\n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(16)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(32)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.lin2  = nn.Linear(7*7*32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        c1 = self.conv1(x)\n",
        "        b1  = self.bn1(c1)\n",
        "        a1  = self.act1(b1)\n",
        "        p1  = self.pool1(a1)\n",
        "\n",
        "        # Layer 2\n",
        "        c2  = self.conv2(p1)\n",
        "        b2  = self.bn2(c2)\n",
        "        a2  = self.act2(b2)\n",
        "        p2  = self.pool2(a2)\n",
        "\n",
        "        # Flatten and Layer 3\n",
        "        flt = p2.view(p2.size(0), -1)\n",
        "        out = self.lin2(flt)\n",
        "        return out\n",
        "  \n",
        "# model = MyConvNet(args)\n",
        "# model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_models(model,out_model):\n",
        "  out_model.conv1 = copy.deepcopy(model.conv1)\n",
        "  out_model.act1 = copy.deepcopy(model.act1)\n",
        "  out_model.pool1 = copy.deepcopy(model.pool1)\n",
        "  out_model.conv2 = copy.deepcopy(model.conv2)\n",
        "  out_model.act2 = copy.deepcopy(model.act2)\n",
        "  out_model.pool2 = copy.deepcopy(model.pool2)\n",
        "  out_model.lin2 = copy.deepcopy(model.lin2)"
      ],
      "metadata": {
        "id": "Zs8kxPMA8gRI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlSJ4lJwXjMh"
      },
      "source": [
        "## Quantization Helper Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "I57BN8KQXjMi"
      },
      "outputs": [],
      "source": [
        "def simple_quantize_val(val, scale_factor, min_val, max_val):\n",
        "  value = torch.round(val / scale_factor)\n",
        "\n",
        "  if (value < min_val):\n",
        "    value = min_val\n",
        "\n",
        "  if (value > max_val):\n",
        "    value = max_val\n",
        "\n",
        "  return (value * scale_factor)\n",
        "\n",
        "def fixed_point_quantize_val(val, num_bits, fractional_bits):\n",
        "  integer_bits = num_bits - fractional_bits - 1 # Subtract one for sign bit\n",
        "  smallest_step_size = 1 / np.power(2, fractional_bits)\n",
        "  largest_number = (np.power(2, integer_bits) - 1) + ((np.power(2, fractional_bits)-1) * smallest_step_size)\n",
        "  smallest_number = -1 * np.power(2, integer_bits)\n",
        "\n",
        "  value = torch.round(val / smallest_step_size) * smallest_step_size\n",
        "\n",
        "  if (value < smallest_number):\n",
        "    value = smallest_number\n",
        "\n",
        "  if (value > largest_number):\n",
        "    value = largest_number\n",
        "\n",
        "  return value\n",
        "\n",
        "# Perhaps slightly optimized version?\n",
        "def fixed_point_quantize_faster(val, smallest_step_size, largest_number, smallest_number):\n",
        "\n",
        "  # Perform Pseudo Quantization\n",
        "  value = torch.round(val / smallest_step_size) * smallest_step_size\n",
        "\n",
        "  # Clamp Values\n",
        "  if (value < smallest_number):\n",
        "    value = smallest_number\n",
        "\n",
        "  if (value > largest_number):\n",
        "    value = largest_number\n",
        "\n",
        "  return value\n",
        "\n",
        "# For flattened and output tensors\n",
        "def quantize_tensor_2d(tens, step_size, largest_num, smallest_num):\n",
        "  shape = tens.shape\n",
        "\n",
        "  # Go through entire tensor and quantize in place\n",
        "  for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "      tens[i][j] = fixed_point_quantize_faster(tens[i][j], step_size, largest_num, smallest_num)\n",
        "\n",
        "# Literally for everything else\n",
        "def quantize_tensor_4d(tens, step_size, largest_num, smallest_num):\n",
        "  shape = tens.shape\n",
        "\n",
        "  for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "      for k in range(shape[2]):\n",
        "        for l in range(shape[3]):\n",
        "          tens[i][j][k][l] = fixed_point_quantize_faster(tens[i][j][k][l], step_size, largest_num, smallest_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLM8kj7KXjMi"
      },
      "source": [
        "## Quantization Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YQn2A92SXjMj"
      },
      "outputs": [],
      "source": [
        "# Assume that we always \n",
        "class MyConvNet_FIXED_POINT(nn.Module):\n",
        "    def __init__(self, args, num_bits, num_fractional_bits):\n",
        "        super(MyConvNet_FIXED_POINT, self).__init__()\n",
        "\n",
        "        # Fixed Point Parameters\n",
        "        self.fp_bits = num_bits\n",
        "        self.sign_bit = 1\n",
        "        self.integer_bits = (num_bits - 1 - num_fractional_bits)\n",
        "        self.fractional_bits = num_fractional_bits\n",
        "\n",
        "        # Fixed Point Computed Values for Quantization\n",
        "        self.smallest_step_size = 1 / np.power(2, num_fractional_bits)\n",
        "        self.largest_num_representable = (np.power(2, self.integer_bits) - 1) + ((np.power(2, self.fractional_bits)-1) * self.smallest_step_size)\n",
        "        self.smallest_num_representable = -1 * np.power(2, self.integer_bits)\n",
        "\n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(16)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(32)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.lin2  = nn.Linear(7*7*32, 10)\n",
        "\n",
        "\n",
        "    # AUGMENT FORWARD PASS. forward pass not quantizes every single time we go through\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        c1 = self.conv1(x)\n",
        "        quantize_tensor_4d(c1, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        b1  = self.bn1(c1)\n",
        "        quantize_tensor_4d(b1, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        a1  = self.act1(b1)\n",
        "        quantize_tensor_4d(a1, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        p1  = self.pool1(a1)\n",
        "        quantize_tensor_4d(p1, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "\n",
        "        # Layer 2\n",
        "        c2  = self.conv2(p1)\n",
        "        quantize_tensor_4d(c2, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        b2  = self.bn2(c2)\n",
        "        quantize_tensor_4d(b2, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        a2  = self.act2(b2)\n",
        "        quantize_tensor_4d(a2, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        p2  = self.pool2(a2)\n",
        "        quantize_tensor_4d(p2, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "\n",
        "        # Flatten and Layer 3\n",
        "        flt = p2.view(p2.size(0), -1)\n",
        "        out = self.lin2(flt)\n",
        "        quantize_tensor_2d(out, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedConvNet(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedConvNet, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        # FP32 model\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8t8Zej6m3ufg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGjcFMQ5XjMj"
      },
      "source": [
        "# Test Code Here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RjT2c_f9XjMk"
      },
      "outputs": [],
      "source": [
        "args = easydict.EasyDict({\n",
        "        \"batch_size\": 1,\n",
        "        \"epochs\": 1,\n",
        "        \"lr\": 0.001,\n",
        "        \"enable_cuda\" : True,\n",
        "        \"L1norm\" : False,\n",
        "        \"simpleNet\" : True,\n",
        "        \"activation\" : \"relu\", #relu, tanh, sigmoid\n",
        "        \"train_curve\" : True, \n",
        "        \"optimization\" :\"SGD\"\n",
        "})\n",
        "\n",
        "# Hyper Parameter for FashionMNIST\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_epochs = args.epochs\n",
        "batch_size = args.batch_size\n",
        "learning_rate = args.lr\n",
        "\n",
        "# Dataset Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_set, batch_size = batch_size, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IlnS6XwxXjMk"
      },
      "outputs": [],
      "source": [
        "# # Declare Model\n",
        "# num_bits = 8\n",
        "# num_fractional_bits = 5\n",
        "# model = MyConvNet_FIXED_POINT(args, num_bits, num_fractional_bits).cuda()\n",
        "# criterion = nn.CrossEntropyLoss().cuda()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
        "\n",
        "# # Training? Commented out for now so that I can just make sure that the forward quantization works.\n",
        "# # train_model(model, criterion, optimizer, train_loader)\n",
        "\n",
        "# # Testing ARC\n",
        "# test_acc, test_loss = get_acc(model, criterion, test_loader)\n",
        "# print(\"Test Accuracy: {}\".format(test_acc))\n",
        "# print(\"Test Loss: {}\".format(test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preston's Training of Model"
      ],
      "metadata": {
        "id": "4z9Q0vi95Alz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_device = torch.device(\"cuda:0\")\n",
        "cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "# Initial model\n",
        "FP_model = MyConvNet_FP32(args).cuda()\n",
        "FP_fused_model = MyConvNet_FP32(args)\n",
        "\n",
        "# Training stuff\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = torch.optim.SGD(FP_model.parameters(), lr = learning_rate) \n",
        "\n",
        "train_model(FP_model, criterion, optimizer, train_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgEQBWHy5FD-",
        "outputId": "1da7aae7-3ea8-4923-af21-b0dc63e07277"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Training started\n",
            "Epoch: [ 1/ 1], Step: [ 600/ 60000], Loss: 1.0428\n",
            "Epoch: [ 1/ 1], Step: [ 1200/ 60000], Loss: 3.5097\n",
            "Epoch: [ 1/ 1], Step: [ 1800/ 60000], Loss: 0.0319\n",
            "Epoch: [ 1/ 1], Step: [ 2400/ 60000], Loss: 0.0356\n",
            "Epoch: [ 1/ 1], Step: [ 3000/ 60000], Loss: 0.0046\n",
            "Epoch: [ 1/ 1], Step: [ 3600/ 60000], Loss: 0.0129\n",
            "Epoch: [ 1/ 1], Step: [ 4200/ 60000], Loss: 0.5654\n",
            "Epoch: [ 1/ 1], Step: [ 4800/ 60000], Loss: 2.1644\n",
            "Epoch: [ 1/ 1], Step: [ 5400/ 60000], Loss: 0.2765\n",
            "Epoch: [ 1/ 1], Step: [ 6000/ 60000], Loss: 0.0235\n",
            "Epoch: [ 1/ 1], Step: [ 6600/ 60000], Loss: 0.0107\n",
            "Epoch: [ 1/ 1], Step: [ 7200/ 60000], Loss: 0.5779\n",
            "Epoch: [ 1/ 1], Step: [ 7800/ 60000], Loss: 0.0000\n",
            "Epoch: [ 1/ 1], Step: [ 8400/ 60000], Loss: 0.2025\n",
            "Epoch: [ 1/ 1], Step: [ 9000/ 60000], Loss: 0.0019\n",
            "Epoch: [ 1/ 1], Step: [ 9600/ 60000], Loss: 0.0054\n",
            "Epoch: [ 1/ 1], Step: [ 10200/ 60000], Loss: 0.0239\n",
            "Epoch: [ 1/ 1], Step: [ 10800/ 60000], Loss: 0.0038\n",
            "Epoch: [ 1/ 1], Step: [ 11400/ 60000], Loss: 0.0009\n",
            "Epoch: [ 1/ 1], Step: [ 12000/ 60000], Loss: 0.0030\n",
            "Epoch: [ 1/ 1], Step: [ 12600/ 60000], Loss: 0.2931\n",
            "Epoch: [ 1/ 1], Step: [ 13200/ 60000], Loss: 0.0844\n",
            "Epoch: [ 1/ 1], Step: [ 13800/ 60000], Loss: 0.3588\n",
            "Epoch: [ 1/ 1], Step: [ 14400/ 60000], Loss: 0.0064\n",
            "Epoch: [ 1/ 1], Step: [ 15000/ 60000], Loss: 0.6209\n",
            "Epoch: [ 1/ 1], Step: [ 15600/ 60000], Loss: 0.0484\n",
            "Epoch: [ 1/ 1], Step: [ 16200/ 60000], Loss: 0.2463\n",
            "Epoch: [ 1/ 1], Step: [ 16800/ 60000], Loss: 0.0052\n",
            "Epoch: [ 1/ 1], Step: [ 17400/ 60000], Loss: 2.0587\n",
            "Epoch: [ 1/ 1], Step: [ 18000/ 60000], Loss: 0.0011\n",
            "Epoch: [ 1/ 1], Step: [ 18600/ 60000], Loss: 0.5029\n",
            "Epoch: [ 1/ 1], Step: [ 19200/ 60000], Loss: 0.2535\n",
            "Epoch: [ 1/ 1], Step: [ 19800/ 60000], Loss: 0.1100\n",
            "Epoch: [ 1/ 1], Step: [ 20400/ 60000], Loss: 0.2048\n",
            "Epoch: [ 1/ 1], Step: [ 21000/ 60000], Loss: 0.0487\n",
            "Epoch: [ 1/ 1], Step: [ 21600/ 60000], Loss: 0.0135\n",
            "Epoch: [ 1/ 1], Step: [ 22200/ 60000], Loss: 0.0050\n",
            "Epoch: [ 1/ 1], Step: [ 22800/ 60000], Loss: 0.0386\n",
            "Epoch: [ 1/ 1], Step: [ 23400/ 60000], Loss: 0.0005\n",
            "Epoch: [ 1/ 1], Step: [ 24000/ 60000], Loss: 0.1885\n",
            "Epoch: [ 1/ 1], Step: [ 24600/ 60000], Loss: 3.7235\n",
            "Epoch: [ 1/ 1], Step: [ 25200/ 60000], Loss: 0.0020\n",
            "Epoch: [ 1/ 1], Step: [ 25800/ 60000], Loss: 2.6182\n",
            "Epoch: [ 1/ 1], Step: [ 26400/ 60000], Loss: 0.1214\n",
            "Epoch: [ 1/ 1], Step: [ 27000/ 60000], Loss: 0.0012\n",
            "Epoch: [ 1/ 1], Step: [ 27600/ 60000], Loss: 1.4443\n",
            "Epoch: [ 1/ 1], Step: [ 28200/ 60000], Loss: 0.1236\n",
            "Epoch: [ 1/ 1], Step: [ 28800/ 60000], Loss: 0.0396\n",
            "Epoch: [ 1/ 1], Step: [ 29400/ 60000], Loss: 0.0041\n",
            "Epoch: [ 1/ 1], Step: [ 30000/ 60000], Loss: 0.0008\n",
            "Epoch: [ 1/ 1], Step: [ 30600/ 60000], Loss: 0.0033\n",
            "Epoch: [ 1/ 1], Step: [ 31200/ 60000], Loss: 0.0925\n",
            "Epoch: [ 1/ 1], Step: [ 31800/ 60000], Loss: 0.0709\n",
            "Epoch: [ 1/ 1], Step: [ 32400/ 60000], Loss: 0.8150\n",
            "Epoch: [ 1/ 1], Step: [ 33000/ 60000], Loss: 0.0003\n",
            "Epoch: [ 1/ 1], Step: [ 33600/ 60000], Loss: 0.0531\n",
            "Epoch: [ 1/ 1], Step: [ 34200/ 60000], Loss: 0.0066\n",
            "Epoch: [ 1/ 1], Step: [ 34800/ 60000], Loss: 0.0076\n",
            "Epoch: [ 1/ 1], Step: [ 35400/ 60000], Loss: 0.0065\n",
            "Epoch: [ 1/ 1], Step: [ 36000/ 60000], Loss: 0.8381\n",
            "Epoch: [ 1/ 1], Step: [ 36600/ 60000], Loss: 0.5323\n",
            "Epoch: [ 1/ 1], Step: [ 37200/ 60000], Loss: 0.1117\n",
            "Epoch: [ 1/ 1], Step: [ 37800/ 60000], Loss: 1.7255\n",
            "Epoch: [ 1/ 1], Step: [ 38400/ 60000], Loss: 0.0015\n",
            "Epoch: [ 1/ 1], Step: [ 39000/ 60000], Loss: 0.0046\n",
            "Epoch: [ 1/ 1], Step: [ 39600/ 60000], Loss: 1.1855\n",
            "Epoch: [ 1/ 1], Step: [ 40200/ 60000], Loss: 0.0098\n",
            "Epoch: [ 1/ 1], Step: [ 40800/ 60000], Loss: 0.0720\n",
            "Epoch: [ 1/ 1], Step: [ 41400/ 60000], Loss: 0.0004\n",
            "Epoch: [ 1/ 1], Step: [ 42000/ 60000], Loss: 0.0462\n",
            "Epoch: [ 1/ 1], Step: [ 42600/ 60000], Loss: 0.0003\n",
            "Epoch: [ 1/ 1], Step: [ 43200/ 60000], Loss: 0.1594\n",
            "Epoch: [ 1/ 1], Step: [ 43800/ 60000], Loss: 0.0039\n",
            "Epoch: [ 1/ 1], Step: [ 44400/ 60000], Loss: 1.4816\n",
            "Epoch: [ 1/ 1], Step: [ 45000/ 60000], Loss: 0.0500\n",
            "Epoch: [ 1/ 1], Step: [ 45600/ 60000], Loss: 0.8417\n",
            "Epoch: [ 1/ 1], Step: [ 46200/ 60000], Loss: 1.5079\n",
            "Epoch: [ 1/ 1], Step: [ 46800/ 60000], Loss: 1.8189\n",
            "Epoch: [ 1/ 1], Step: [ 47400/ 60000], Loss: 0.1993\n",
            "Epoch: [ 1/ 1], Step: [ 48000/ 60000], Loss: 0.0175\n",
            "Epoch: [ 1/ 1], Step: [ 48600/ 60000], Loss: 0.0278\n",
            "Epoch: [ 1/ 1], Step: [ 49200/ 60000], Loss: 0.0065\n",
            "Epoch: [ 1/ 1], Step: [ 49800/ 60000], Loss: 0.0224\n",
            "Epoch: [ 1/ 1], Step: [ 50400/ 60000], Loss: 0.7828\n",
            "Epoch: [ 1/ 1], Step: [ 51000/ 60000], Loss: 0.0358\n",
            "Epoch: [ 1/ 1], Step: [ 51600/ 60000], Loss: 1.2092\n",
            "Epoch: [ 1/ 1], Step: [ 52200/ 60000], Loss: 0.0102\n",
            "Epoch: [ 1/ 1], Step: [ 52800/ 60000], Loss: 0.0115\n",
            "Epoch: [ 1/ 1], Step: [ 53400/ 60000], Loss: 0.0116\n",
            "Epoch: [ 1/ 1], Step: [ 54000/ 60000], Loss: 0.3661\n",
            "Epoch: [ 1/ 1], Step: [ 54600/ 60000], Loss: 0.0937\n",
            "Epoch: [ 1/ 1], Step: [ 55200/ 60000], Loss: 0.0064\n",
            "Epoch: [ 1/ 1], Step: [ 55800/ 60000], Loss: 1.0838\n",
            "Epoch: [ 1/ 1], Step: [ 56400/ 60000], Loss: 0.0339\n",
            "Epoch: [ 1/ 1], Step: [ 57000/ 60000], Loss: 0.0017\n",
            "Epoch: [ 1/ 1], Step: [ 57600/ 60000], Loss: 0.0058\n",
            "Epoch: [ 1/ 1], Step: [ 58200/ 60000], Loss: 0.0021\n",
            "Epoch: [ 1/ 1], Step: [ 58800/ 60000], Loss: 0.0024\n",
            "Epoch: [ 1/ 1], Step: [ 59400/ 60000], Loss: 0.0434\n",
            "Epoch: [ 1/ 1], Step: [ 60000/ 60000], Loss: 0.0192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc_FP, test_loss_FP = get_acc(FP_model, criterion, test_loader)\n",
        "print(\"Test Accuracy: {}\".format(test_acc_FP))\n",
        "print(\"Test Loss: {}\".format(test_loss_FP))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P63UTvuTHjKT",
        "outputId": "3f89fc70-6914-459b-9fb8-c92de9d4b416"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 100.0\n",
            "Test Loss: 0.0013071097200736403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "copy_models(FP_model,FP_fused_model)   \n",
        "fuse_conv_bn(FP_model,FP_fused_model)\n",
        "\n",
        "quantized_model = QuantizedConvNet(model_fp32=FP_fused_model).to(cpu_device)\n",
        "\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "quantized_model.qconfig = quantization_config\n",
        "torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
        "\n",
        "quantized_model.to(cuda_device)\n",
        "train_model(quantized_model, criterion, optimizer, train_loader)\n",
        "# quantized_model.to(cpu_device)\n",
        "\n",
        "# quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJYZN0ky_9i6",
        "outputId": "52bc19b3-7d80-4ed3-e0ca-c94af2cfc865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:178: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Training started\n",
            "Epoch: [ 1/ 1], Step: [ 600/ 60000], Loss: 2.0750\n",
            "Epoch: [ 1/ 1], Step: [ 1200/ 60000], Loss: 0.5288\n",
            "Epoch: [ 1/ 1], Step: [ 1800/ 60000], Loss: 0.1383\n",
            "Epoch: [ 1/ 1], Step: [ 2400/ 60000], Loss: 0.0011\n",
            "Epoch: [ 1/ 1], Step: [ 3000/ 60000], Loss: 1.8018\n",
            "Epoch: [ 1/ 1], Step: [ 3600/ 60000], Loss: 0.0420\n",
            "Epoch: [ 1/ 1], Step: [ 4200/ 60000], Loss: 0.1006\n",
            "Epoch: [ 1/ 1], Step: [ 4800/ 60000], Loss: 0.0049\n",
            "Epoch: [ 1/ 1], Step: [ 5400/ 60000], Loss: 3.9848\n",
            "Epoch: [ 1/ 1], Step: [ 6000/ 60000], Loss: 0.0170\n",
            "Epoch: [ 1/ 1], Step: [ 6600/ 60000], Loss: 0.0018\n",
            "Epoch: [ 1/ 1], Step: [ 7200/ 60000], Loss: 0.0130\n",
            "Epoch: [ 1/ 1], Step: [ 7800/ 60000], Loss: 0.0149\n",
            "Epoch: [ 1/ 1], Step: [ 8400/ 60000], Loss: 0.9784\n",
            "Epoch: [ 1/ 1], Step: [ 9000/ 60000], Loss: 0.0049\n",
            "Epoch: [ 1/ 1], Step: [ 9600/ 60000], Loss: 4.7764\n",
            "Epoch: [ 1/ 1], Step: [ 10200/ 60000], Loss: 0.0011\n",
            "Epoch: [ 1/ 1], Step: [ 10800/ 60000], Loss: 3.8788\n",
            "Epoch: [ 1/ 1], Step: [ 11400/ 60000], Loss: 0.2645\n",
            "Epoch: [ 1/ 1], Step: [ 12000/ 60000], Loss: 0.0096\n",
            "Epoch: [ 1/ 1], Step: [ 12600/ 60000], Loss: 0.0004\n",
            "Epoch: [ 1/ 1], Step: [ 13200/ 60000], Loss: 0.0033\n",
            "Epoch: [ 1/ 1], Step: [ 13800/ 60000], Loss: 1.2192\n",
            "Epoch: [ 1/ 1], Step: [ 14400/ 60000], Loss: 0.0020\n",
            "Epoch: [ 1/ 1], Step: [ 15000/ 60000], Loss: 0.0542\n",
            "Epoch: [ 1/ 1], Step: [ 15600/ 60000], Loss: 0.6132\n",
            "Epoch: [ 1/ 1], Step: [ 16200/ 60000], Loss: 0.2947\n",
            "Epoch: [ 1/ 1], Step: [ 16800/ 60000], Loss: 0.0002\n",
            "Epoch: [ 1/ 1], Step: [ 17400/ 60000], Loss: 0.0455\n",
            "Epoch: [ 1/ 1], Step: [ 18000/ 60000], Loss: 0.0029\n",
            "Epoch: [ 1/ 1], Step: [ 18600/ 60000], Loss: 0.4593\n",
            "Epoch: [ 1/ 1], Step: [ 19200/ 60000], Loss: 0.0323\n",
            "Epoch: [ 1/ 1], Step: [ 19800/ 60000], Loss: 0.0003\n",
            "Epoch: [ 1/ 1], Step: [ 20400/ 60000], Loss: 0.0003\n",
            "Epoch: [ 1/ 1], Step: [ 21000/ 60000], Loss: 0.3016\n",
            "Epoch: [ 1/ 1], Step: [ 21600/ 60000], Loss: 2.0618\n",
            "Epoch: [ 1/ 1], Step: [ 22200/ 60000], Loss: 0.7252\n",
            "Epoch: [ 1/ 1], Step: [ 22800/ 60000], Loss: 0.0022\n",
            "Epoch: [ 1/ 1], Step: [ 23400/ 60000], Loss: 0.0005\n",
            "Epoch: [ 1/ 1], Step: [ 24000/ 60000], Loss: 0.0009\n",
            "Epoch: [ 1/ 1], Step: [ 24600/ 60000], Loss: 0.0182\n",
            "Epoch: [ 1/ 1], Step: [ 25200/ 60000], Loss: 3.6327\n",
            "Epoch: [ 1/ 1], Step: [ 25800/ 60000], Loss: 0.0589\n",
            "Epoch: [ 1/ 1], Step: [ 26400/ 60000], Loss: 0.0476\n",
            "Epoch: [ 1/ 1], Step: [ 27000/ 60000], Loss: 0.0002\n",
            "Epoch: [ 1/ 1], Step: [ 27600/ 60000], Loss: 0.0002\n",
            "Epoch: [ 1/ 1], Step: [ 28200/ 60000], Loss: 0.0272\n",
            "Epoch: [ 1/ 1], Step: [ 28800/ 60000], Loss: 0.0002\n",
            "Epoch: [ 1/ 1], Step: [ 29400/ 60000], Loss: 0.0045\n",
            "Epoch: [ 1/ 1], Step: [ 30000/ 60000], Loss: 0.0035\n",
            "Epoch: [ 1/ 1], Step: [ 30600/ 60000], Loss: 0.0113\n",
            "Epoch: [ 1/ 1], Step: [ 31200/ 60000], Loss: 0.6726\n",
            "Epoch: [ 1/ 1], Step: [ 31800/ 60000], Loss: 5.9757\n",
            "Epoch: [ 1/ 1], Step: [ 32400/ 60000], Loss: 0.0032\n",
            "Epoch: [ 1/ 1], Step: [ 33000/ 60000], Loss: 1.1428\n",
            "Epoch: [ 1/ 1], Step: [ 33600/ 60000], Loss: 0.8032\n",
            "Epoch: [ 1/ 1], Step: [ 34200/ 60000], Loss: 2.0400\n",
            "Epoch: [ 1/ 1], Step: [ 34800/ 60000], Loss: 0.0017\n",
            "Epoch: [ 1/ 1], Step: [ 35400/ 60000], Loss: 1.9422\n",
            "Epoch: [ 1/ 1], Step: [ 36000/ 60000], Loss: 0.0006\n",
            "Epoch: [ 1/ 1], Step: [ 36600/ 60000], Loss: 0.0052\n",
            "Epoch: [ 1/ 1], Step: [ 37200/ 60000], Loss: 0.0000\n",
            "Epoch: [ 1/ 1], Step: [ 37800/ 60000], Loss: 0.0395\n",
            "Epoch: [ 1/ 1], Step: [ 38400/ 60000], Loss: 0.0053\n",
            "Epoch: [ 1/ 1], Step: [ 39000/ 60000], Loss: 0.0012\n",
            "Epoch: [ 1/ 1], Step: [ 39600/ 60000], Loss: 0.2557\n",
            "Epoch: [ 1/ 1], Step: [ 40200/ 60000], Loss: 0.2445\n",
            "Epoch: [ 1/ 1], Step: [ 40800/ 60000], Loss: 0.0017\n",
            "Epoch: [ 1/ 1], Step: [ 41400/ 60000], Loss: 0.0089\n",
            "Epoch: [ 1/ 1], Step: [ 42000/ 60000], Loss: 0.0104\n",
            "Epoch: [ 1/ 1], Step: [ 42600/ 60000], Loss: 0.0016\n",
            "Epoch: [ 1/ 1], Step: [ 43200/ 60000], Loss: 0.0441\n",
            "Epoch: [ 1/ 1], Step: [ 43800/ 60000], Loss: 0.0174\n",
            "Epoch: [ 1/ 1], Step: [ 44400/ 60000], Loss: 0.0036\n",
            "Epoch: [ 1/ 1], Step: [ 45000/ 60000], Loss: 0.0106\n",
            "Epoch: [ 1/ 1], Step: [ 45600/ 60000], Loss: 0.0013\n",
            "Epoch: [ 1/ 1], Step: [ 46200/ 60000], Loss: 0.0003\n",
            "Epoch: [ 1/ 1], Step: [ 46800/ 60000], Loss: 0.0561\n",
            "Epoch: [ 1/ 1], Step: [ 47400/ 60000], Loss: 0.4617\n",
            "Epoch: [ 1/ 1], Step: [ 48000/ 60000], Loss: 0.0031\n",
            "Epoch: [ 1/ 1], Step: [ 48600/ 60000], Loss: 0.0052\n",
            "Epoch: [ 1/ 1], Step: [ 49200/ 60000], Loss: 0.5602\n",
            "Epoch: [ 1/ 1], Step: [ 49800/ 60000], Loss: 0.0001\n",
            "Epoch: [ 1/ 1], Step: [ 50400/ 60000], Loss: 0.0228\n",
            "Epoch: [ 1/ 1], Step: [ 51000/ 60000], Loss: 0.4735\n",
            "Epoch: [ 1/ 1], Step: [ 51600/ 60000], Loss: 0.0007\n",
            "Epoch: [ 1/ 1], Step: [ 52200/ 60000], Loss: 0.4319\n",
            "Epoch: [ 1/ 1], Step: [ 52800/ 60000], Loss: 0.0019\n",
            "Epoch: [ 1/ 1], Step: [ 53400/ 60000], Loss: 0.0024\n",
            "Epoch: [ 1/ 1], Step: [ 54000/ 60000], Loss: 1.2700\n",
            "Epoch: [ 1/ 1], Step: [ 54600/ 60000], Loss: 0.0057\n",
            "Epoch: [ 1/ 1], Step: [ 55200/ 60000], Loss: 0.0006\n",
            "Epoch: [ 1/ 1], Step: [ 55800/ 60000], Loss: 0.0037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_acc, test_loss = get_acc(quantized_model, criterion, test_loader)\n",
        "print(\"Test Accuracy: {}\".format(test_acc))\n",
        "print(\"Test Loss: {}\".format(test_loss))\n",
        "\n"
      ],
      "metadata": {
        "id": "QhdEVwvJHnhl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}