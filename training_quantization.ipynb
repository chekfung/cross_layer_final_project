{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chekfung/cross_layer_final_project/blob/main/training_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWCSJNoXXjMU"
      },
      "source": [
        "# Source Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJuqknGwXjMW"
      },
      "source": [
        "## Imports Needed Throughout the Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "B-AzI0DdXjMY"
      },
      "outputs": [],
      "source": [
        "# All Imports \n",
        "import sys\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from google.colab import files\n",
        "\n",
        "# argument parser\n",
        "import easydict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItgEiUWPXjMa"
      },
      "source": [
        "## Get and Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "wWPbK4uKXjMb"
      },
      "outputs": [],
      "source": [
        "# MNIST Dataset (Images and Labels)\n",
        "train_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()                                 \n",
        "    ])\n",
        ")\n",
        "\n",
        "test_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()                                 \n",
        "    ])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOYytU77XjMc"
      },
      "source": [
        "## More Helper Code for Training and Testing Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def fuse_conv_bn(model,out_model):\n",
        "\n",
        "  conv_layer = None\n",
        "  count = 0\n",
        "\n",
        "  # 1. for loop to collect all Conv layers\n",
        "  # 2. for loop to collect all BatchNorm layers\n",
        "  for layer in model.modules():\n",
        "    \n",
        "    if isinstance(layer, nn.BatchNorm2d):\n",
        "\n",
        "      conv_size = conv_layer.weight.size()\n",
        "\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for i in range(conv_size[0]):\n",
        "\n",
        "          \n",
        "          # get the conv2d weights\n",
        "          weights = conv_layer.weight[i]\n",
        "\n",
        "\n",
        "          denominator = torch.sqrt(layer.eps+layer.running_var)\n",
        "          gamma = layer.weight[i]\n",
        "          beta = layer.bias[i]\n",
        "          \n",
        "\n",
        "          for j in range(conv_size[1]):\n",
        "            for k in range(conv_size[2]):\n",
        "              for l in range(conv_size[3]):\n",
        "                # update out_model layer[count]\n",
        "\n",
        "                if count == 0:\n",
        "                  out_model.conv1.weight[i][j][k][l] = gamma * conv_layer.weight[i][j][k][l]  / denominator[i]  \n",
        "                else:\n",
        "                  out_model.conv2.weight[i][j][k][l] = gamma * conv_layer.weight[i][j][k][l]  / denominator[i]  \n",
        "\n",
        "          # In i loop for bias since only 1D\n",
        "          if count == 0:\n",
        "            out_model.conv1.bias[i] = (gamma * (conv_layer.bias[i] - layer.running_mean[i])  / denominator[i]) + beta\n",
        "          else:\n",
        "            out_model.conv2.bias[i] = (gamma * (conv_layer.bias[i] - layer.running_mean[i])  / denominator[i]) + beta\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    conv_layer = layer # conv2d\n"
      ],
      "metadata": {
        "id": "QcNG3HYC7dJk"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)"
      ],
      "metadata": {
        "id": "q3PX3CO2228h"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzSo2kcrXjMe"
      },
      "source": [
        "## FP32 Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "vlRcw7IMXjMg"
      },
      "outputs": [],
      "source": [
        "class MyConvNet_FP32(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MyConvNet_FP32, self).__init__()\n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(16)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(32)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.lin2  = nn.Linear(7*7*32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        c1 = self.conv1(x)\n",
        "        b1  = self.bn1(c1)\n",
        "        a1  = self.act1(b1)\n",
        "        p1  = self.pool1(a1)\n",
        "\n",
        "        # Layer 2\n",
        "        c2  = self.conv2(p1)\n",
        "        b2  = self.bn2(c2)\n",
        "        a2  = self.act2(b2)\n",
        "        p2  = self.pool2(a2)\n",
        "\n",
        "        # Flatten and Layer 3\n",
        "        flt = p2.view(p2.size(0), -1)\n",
        "        out = self.lin2(flt)\n",
        "        return out\n",
        "  \n",
        "# model = MyConvNet(args)\n",
        "# model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_models(model,out_model):\n",
        "  out_model.conv1 = copy.deepcopy(model.conv1)\n",
        "  out_model.act1 = copy.deepcopy(model.act1)\n",
        "  out_model.pool1 = copy.deepcopy(model.pool1)\n",
        "  out_model.conv2 = copy.deepcopy(model.conv2)\n",
        "  out_model.act2 = copy.deepcopy(model.act2)\n",
        "  out_model.pool2 = copy.deepcopy(model.pool2)\n",
        "  out_model.lin2 = copy.deepcopy(model.lin2)"
      ],
      "metadata": {
        "id": "Zs8kxPMA8gRI"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlSJ4lJwXjMh"
      },
      "source": [
        "## Quantization Helper Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "I57BN8KQXjMi"
      },
      "outputs": [],
      "source": [
        "def simple_quantize_val(val, scale_factor, min_val, max_val):\n",
        "  value = torch.round(val / scale_factor)\n",
        "\n",
        "  if (value < min_val):\n",
        "    value = min_val\n",
        "\n",
        "  if (value > max_val):\n",
        "    value = max_val\n",
        "\n",
        "  return (value * scale_factor)\n",
        "\n",
        "def fixed_point_quantize_val(val, num_bits, fractional_bits):\n",
        "  integer_bits = num_bits - fractional_bits - 1 # Subtract one for sign bit\n",
        "  smallest_step_size = 1 / np.power(2, fractional_bits)\n",
        "  largest_number = (np.power(2, integer_bits) - 1) + ((np.power(2, fractional_bits)-1) * smallest_step_size)\n",
        "  smallest_number = -1 * np.power(2, integer_bits)\n",
        "\n",
        "  value = torch.round(val / smallest_step_size) * smallest_step_size\n",
        "\n",
        "  if (value < smallest_number):\n",
        "    value = smallest_number\n",
        "\n",
        "  if (value > largest_number):\n",
        "    value = largest_number\n",
        "\n",
        "  return value\n",
        "\n",
        "# Perhaps slightly optimized version?\n",
        "def fixed_point_quantize_faster(val, smallest_step_size, largest_number, smallest_number):\n",
        "\n",
        "  # Perform Pseudo Quantization\n",
        "  value = torch.round(val / smallest_step_size) * smallest_step_size\n",
        "\n",
        "  # Clamp Values\n",
        "  if (value < smallest_number):\n",
        "    value = smallest_number\n",
        "\n",
        "  if (value > largest_number):\n",
        "    value = largest_number\n",
        "\n",
        "  return value\n",
        "\n",
        "# For flattened and output tensors\n",
        "def quantize_tensor_1d(tens, step_size, largest_num, smallest_num):\n",
        "  shape = tens.shape\n",
        "  new_tensor = torch.zeros(shape).cuda()\n",
        "\n",
        "  # Go through entire tensor and quantize in place\n",
        "  for i in range(shape[0]):\n",
        "      new_tensor[i] = fixed_point_quantize_faster(tens[i], step_size, largest_num, smallest_num)\n",
        "  return new_tensor\n",
        "\n",
        "# For flattened and output tensors\n",
        "def quantize_tensor_2d(tens, step_size, largest_num, smallest_num):\n",
        "  shape = tens.shape\n",
        "  new_tensor = torch.zeros(shape).cuda()\n",
        "\n",
        "  # Go through entire tensor and quantize in place\n",
        "  for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "      new_tensor[i][j] = fixed_point_quantize_faster(tens[i][j], step_size, largest_num, smallest_num)\n",
        "  return new_tensor\n",
        "\n",
        "# Literally for everything else\n",
        "def quantize_tensor_4d(tens, step_size, largest_num, smallest_num):\n",
        "  shape = tens.shape\n",
        "  new_tensor = torch.zeros(shape).cuda()\n",
        "\n",
        "\n",
        "  for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "      for k in range(shape[2]):\n",
        "        for l in range(shape[3]):\n",
        "          new_tensor[i][j][k][l] = fixed_point_quantize_faster(tens[i][j][k][l], step_size, largest_num, smallest_num)\n",
        "  return new_tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simple_quantize_val(val, scale_factor, min_val, max_val):\n",
        "  value = torch.round(val / scale_factor)\n",
        "\n",
        "  if (value < min_val):\n",
        "    value = min_val\n",
        "\n",
        "  if (value > max_val):\n",
        "    value = max_val\n",
        "\n",
        "  return (value * scale_factor)\n",
        "\n",
        "def fixed_point_quantize_val(val, num_bits, fractional_bits):\n",
        "  integer_bits = num_bits - fractional_bits - 1 # Subtract one for sign bit\n",
        "  smallest_step_size = 1 / np.power(2, fractional_bits)\n",
        "  largest_number = (np.power(2, integer_bits) - 1) + ((np.power(2, fractional_bits)-1) * smallest_step_size)\n",
        "  smallest_number = -1 * np.power(2, integer_bits)\n",
        "\n",
        "  value = torch.round(val / smallest_step_size) * smallest_step_size\n",
        "\n",
        "  if (value < smallest_number):\n",
        "    value = smallest_number\n",
        "\n",
        "  if (value > largest_number):\n",
        "    value = largest_number\n",
        "\n",
        "  return value\n",
        "\n",
        "\n",
        "# Gets global min and max\n",
        "def get_min_max_weight_val(model):\n",
        "  cnt = 0\n",
        "  global_max = -np.inf\n",
        "  global_min = np.inf\n",
        "\n",
        "  # Loop through layers and get global min and max of weights\n",
        "  for layer in model.modules():\n",
        "    if not isinstance(layer, (nn.ReLU, nn.MaxPool2d))and cnt != 0:\n",
        "      local_max = torch.max(layer.weight).data\n",
        "      local_min = torch.min(layer.weight).data\n",
        "\n",
        "      if local_max > global_max:\n",
        "        global_max = local_max\n",
        "      \n",
        "      if local_min < global_min:\n",
        "        global_min = local_min\n",
        "\n",
        "    cnt+=1\n",
        "\n",
        "  return global_max, global_min \n",
        "\n",
        "# Right now, only for integer quantization.\n",
        "def quantize_model(model,num_bits, fixed_point_bool, num_fractional_bits):\n",
        "\n",
        "  # max_val, min_val = get_min_max_weight_val(load_model)\n",
        "  max_val, min_val = get_min_max_weight_val(model)\n",
        "\n",
        "  # Assume at the moment, signed ints at the moment.\n",
        "  quantize_range = np.power(2, num_bits) - 1\n",
        "  quantize_min_val = -1 * np.power(2, num_bits - 1)\n",
        "  quantize_max_val = np.power(2, num_bits-1)-1\n",
        "\n",
        "  weight_range = max_val - min_val\n",
        "  scale_factor = weight_range / quantize_range\n",
        "  zero_point = torch.round(torch.abs(min_val) / scale_factor)\n",
        "\n",
        "  count = 0\n",
        "  for layer in model.modules():\n",
        "      if not isinstance(layer, (nn.ReLU, nn.MaxPool2d)) and count != 0:\n",
        "\n",
        "        layer_shape = layer.weight.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if isinstance(layer, nn.Conv2d):\n",
        "            for i in range(layer_shape[0]):\n",
        "              for j in range(layer_shape[1]):\n",
        "                for k in range(layer_shape[2]):\n",
        "                  for l in range(layer_shape[3]):\n",
        "                    if not fixed_point_bool:\n",
        "                      layer.weight[i][j][k][l] = simple_quantize_val(layer.weight[i][j][k][l], scale_factor, quantize_min_val, quantize_max_val)\n",
        "                    else:\n",
        "                      layer.weight[i][j][k][l] = fixed_point_quantize_val(layer.weight[i][j][k][l], num_bits, num_fractional_bits)\n",
        "\n",
        "          if isinstance(layer, nn.BatchNorm2d):\n",
        "            for i in range(layer_shape[0]):\n",
        "              if not fixed_point_bool:\n",
        "                layer.weight[i] = simple_quantize_val(layer.weight[i], scale_factor, quantize_min_val, quantize_max_val)\n",
        "              else:\n",
        "                layer.weight[i] = fixed_point_quantize_val(layer.weight[i], num_bits, num_fractional_bits)\n",
        "\n",
        "          if isinstance(layer, nn.Linear):\n",
        "            for i in range(layer_shape[0]):\n",
        "              for j in range(layer_shape[1]):\n",
        "                if not fixed_point_bool:\n",
        "                  layer.weight[i][j] = simple_quantize_val(layer.weight[i][j], scale_factor, quantize_min_val, quantize_max_val)\n",
        "                else:\n",
        "                  layer.weight[i][j] = fixed_point_quantize_val(layer.weight[i][j], num_bits, num_fractional_bits)\n",
        "\n",
        "      count += 1\n",
        "  \n",
        "  return 0"
      ],
      "metadata": {
        "id": "eKMviCrSpVIe"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "ln6bdN1fXjMd"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, quantize=False):\n",
        "    print(\"---Training started\")\n",
        "    # Training the Model\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Load Images into GPU\n",
        "            images = images.cuda()\n",
        "            labels = Variable(labels).cuda()\n",
        "\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            L1norm = model.parameters()\n",
        "            arr = []\n",
        "\n",
        "            # Calculate L1 Norm (if included in hyperparameters)\n",
        "            if args.L1norm == True:\n",
        "                for name,param in model.named_parameters():\n",
        "                    if 'weight' in name.split('.'):\n",
        "                        arr.append(param)\n",
        "\n",
        "                L1loss = 0\n",
        "                for Losstmp in arr:\n",
        "                    L1loss = L1loss+Losstmp.abs().mean()\n",
        "\n",
        "                if len(arr) > 0:\n",
        "                    loss = loss+L1loss/len(arr)\n",
        "\n",
        "            # if quantize:\n",
        "            #     # quantize loss\n",
        "            #     # TODO: Explore how to quantize loss\n",
        "            #     loss = fixed_point_quantize_faster(loss,model.smallest_step_size,model.largest_num_representable, model.smallest_num_representable)\n",
        "\n",
        "            # Optimizer Step, Propagate Loss backwards\n",
        "            loss.backward()\n",
        "\n",
        "            if quantize:\n",
        "                # quantize gradients\n",
        "\n",
        "                for name,param in model.named_parameters():\n",
        "                    gradient = param.grad\n",
        "                    \n",
        "                    # Print BEFORE Gradients\n",
        "                    # print(gradient)\n",
        "\n",
        "\n",
        "                    if len(gradient.shape) == 4:\n",
        "                      param.grad = quantize_tensor_4d(gradient, model.smallest_step_size, model.largest_num_representable, model.smallest_num_representable)\n",
        "                    elif len(gradient.shape) == 2:\n",
        "                      param.grad = quantize_tensor_2d(gradient, model.smallest_step_size, model.largest_num_representable, model.smallest_num_representable)\n",
        "                    elif len(gradient.shape) == 1:\n",
        "                      param.grad = quantize_tensor_1d(gradient, model.smallest_step_size, model.largest_num_representable, model.smallest_num_representable)\n",
        "                    else:\n",
        "                      print(\"Shape doesn't match 4, 2, or 1: \",gradient.shape)\n",
        "                    \n",
        "                    # Print AFTER Gradients\n",
        "                    # print(gradient)\n",
        "                \n",
        "            optimizer.step()\n",
        "            # quantize model\n",
        "            fixed_point_bool = True\n",
        "            quantize_model(model,model.fp_bits, fixed_point_bool, model.fractional_bits)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            if (i + 1) % 600 == 0:\n",
        "                print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                        % (epoch + 1, num_epochs, i + 1,\n",
        "                        len(train_set) // batch_size, loss.data.item()))\n",
        "                return\n",
        "\n",
        "\n",
        "# Gets accuracy given dataset as well as total test loss\n",
        "def get_acc(model, criterion, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = model(images)\n",
        "        testloss = criterion(outputs, labels)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "        break\n",
        "\n",
        "    return ((100 * correct / total), testloss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLM8kj7KXjMi"
      },
      "source": [
        "## Quantization Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "YQn2A92SXjMj"
      },
      "outputs": [],
      "source": [
        "# Assume that we always \n",
        "class MyConvNet_FIXED_POINT(nn.Module):\n",
        "    def __init__(self, args, num_bits, num_fractional_bits):\n",
        "        super(MyConvNet_FIXED_POINT, self).__init__()\n",
        "\n",
        "        # Fixed Point Parameters\n",
        "        self.fp_bits = num_bits\n",
        "        self.sign_bit = 1\n",
        "        self.integer_bits = (num_bits - 1 - num_fractional_bits)\n",
        "        self.fractional_bits = num_fractional_bits\n",
        "\n",
        "        # Fixed Point Computed Values for Quantization\n",
        "        self.smallest_step_size = 1 / np.power(2, num_fractional_bits)\n",
        "        self.largest_num_representable = (np.power(2, self.integer_bits) - 1) + ((np.power(2, self.fractional_bits)-1) * self.smallest_step_size)\n",
        "        self.smallest_num_representable = -1 * np.power(2, self.integer_bits)\n",
        "\n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(16)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(32)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Layer 3\n",
        "        self.lin2  = nn.Linear(7*7*32, 10)\n",
        "\n",
        "\n",
        "    # AUGMENT FORWARD PASS. forward pass not quantizes every single time we go through\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        c1 = self.conv1(x)\n",
        "        c1q = quantize_tensor_4d(c1, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        b1  = self.bn1(c1q)\n",
        "        b1q = quantize_tensor_4d(b1, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        a1  = self.act1(b1q)\n",
        "        a1q = quantize_tensor_4d(a1, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        p1  = self.pool1(a1q)\n",
        "        p1q = quantize_tensor_4d(p1, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "\n",
        "        # Layer 2\n",
        "        c2  = self.conv2(p1q)\n",
        "        c2q = quantize_tensor_4d(c2, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        b2  = self.bn2(c2q)\n",
        "        b2q = quantize_tensor_4d(b2, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        a2  = self.act2(b2q)\n",
        "        a2q = quantize_tensor_4d(a2, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        p2  = self.pool2(a2q)\n",
        "        p2q = quantize_tensor_4d(p2, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "\n",
        "        # Flatten and Layer 3\n",
        "        flt = p2.view(p2q.size(0), -1)\n",
        "        out = self.lin2(flt)\n",
        "        out_new = quantize_tensor_2d(out, self.smallest_step_size, self.largest_num_representable, self.smallest_num_representable)\n",
        "        return out_new"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedConvNet(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedConvNet, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        # FP32 model\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8t8Zej6m3ufg"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGjcFMQ5XjMj"
      },
      "source": [
        "# Test Code Here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "RjT2c_f9XjMk"
      },
      "outputs": [],
      "source": [
        "args = easydict.EasyDict({\n",
        "        \"batch_size\": 1,\n",
        "        \"epochs\": 1,\n",
        "        \"lr\": 0.001,\n",
        "        \"enable_cuda\" : True,\n",
        "        \"L1norm\" : False,\n",
        "        \"simpleNet\" : True,\n",
        "        \"activation\" : \"relu\", #relu, tanh, sigmoid\n",
        "        \"train_curve\" : True, \n",
        "        \"optimization\" :\"SGD\"\n",
        "})\n",
        "\n",
        "# Hyper Parameter for FashionMNIST\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_epochs = args.epochs\n",
        "batch_size = args.batch_size\n",
        "learning_rate = args.lr\n",
        "\n",
        "# Dataset Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_set, batch_size = batch_size, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlnS6XwxXjMk",
        "outputId": "c1597091-f72b-403e-b3c5-273671b4466b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Training started\n"
          ]
        }
      ],
      "source": [
        "# Declare Model\n",
        "num_bits = 8\n",
        "num_fractional_bits = 5\n",
        "model = MyConvNet_FIXED_POINT(args, num_bits, num_fractional_bits).cuda()\n",
        "# model = MyConvNet_FP32(args).cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
        "\n",
        "# Training? Commented out for now so that I can just make sure that the forward quantization works.\n",
        "train_model(model, criterion, optimizer, train_loader, True)\n",
        "\n",
        "# Testing ARC\n",
        "test_acc, test_loss = get_acc(model, criterion, test_loader)\n",
        "print(\"Test Accuracy: {}\".format(test_acc))\n",
        "print(\"Test Loss: {}\".format(test_loss))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preston's Training of Model"
      ],
      "metadata": {
        "id": "4z9Q0vi95Alz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cuda_device = torch.device(\"cuda:0\")\n",
        "# cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "# # Initial model\n",
        "# FP_model = MyConvNet_FP32(args).cuda()\n",
        "# FP_fused_model = MyConvNet_FP32(args)\n",
        "\n",
        "# # Training stuff\n",
        "# criterion = nn.CrossEntropyLoss().cuda()\n",
        "# optimizer = torch.optim.SGD(FP_model.parameters(), lr = learning_rate) \n",
        "\n",
        "# train_model(FP_model, criterion, optimizer, train_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "fgEQBWHy5FD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_acc_FP, test_loss_FP = get_acc(FP_model, criterion, test_loader)\n",
        "# print(\"Test Accuracy: {}\".format(test_acc_FP))\n",
        "# print(\"Test Loss: {}\".format(test_loss_FP))\n"
      ],
      "metadata": {
        "id": "P63UTvuTHjKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# copy_models(FP_model,FP_fused_model)   \n",
        "# fuse_conv_bn(FP_model,FP_fused_model)\n",
        "\n",
        "# quantized_model = QuantizedConvNet(model_fp32=FP_fused_model).to(cpu_device)\n",
        "\n",
        "# quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "# quantized_model.qconfig = quantization_config\n",
        "# torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
        "\n",
        "# quantized_model.to(cuda_device)\n",
        "# train_model(quantized_model, criterion, optimizer, train_loader)\n",
        "# # quantized_model.to(cpu_device)\n",
        "\n",
        "# # quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "rJYZN0ky_9i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# test_acc, test_loss = get_acc(quantized_model, criterion, test_loader)\n",
        "# print(\"Test Accuracy: {}\".format(test_acc))\n",
        "# print(\"Test Loss: {}\".format(test_loss))\n",
        "\n"
      ],
      "metadata": {
        "id": "QhdEVwvJHnhl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}